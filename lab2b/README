NAME: William Tan
EMAIL: Willtan510@gmail.com


lab2_list.c
The code that implements the parallel threads to be able to work concurrently on multiple sublists of a single list. 

Makefile
Builds the executable, runs the tests, graphs the plots, profiles the execution data, and creates the tarball

lab2b_list.csv makeList.sh graph.gp
Runs test cases and puts the data into the csv file. Graphs this data to produce necessary plots

profile.out
Report showing the execution profiling of the performance in the unpartitioned spin-lock implementation

SortedList.c SortedList.h
Code defining necessary functions to manipulate the linked lists used in the lab2_list.c file.

.png files
Depicts different graphs illustrating different aspects of the lab2_list program performance.

Question Answers
2.3.1
In the 1 and 2-thread list tests, most of the cycles are probably spent within the critical sections of the code. These are probably the most expensive parts of the code when the thread number is as low as this because there is not as much competition for access to the critical section so there isn't much overhead to clog the throughput. Most of the time/cycles spent in high-thread spin-lock tests is in the threads spinning until the end of their time slices if they don't have access to the critical section. With a high number of threads, this spinning is a giant waste of resources. With high-thread mutex tests, a lot of time is wasted when a system call is used to put the thread to sleep until the lock is available. Because system calls are expensive and more threads means more competiton and thus more system calls, this just leads to a throughput disaster.

2.3.2
The lines of code consuming the most cycles when there are a large number of threads is unsurprisingly the while loop for __sync_lock_test_and_set. This operation becomes so expensive with large number of threads because there are more time slices to be completely used as the locks spin until they get the lock.

2.3.3
The average lock-wait time rises with the number of contending threads because as there are more threads, there is the same single lock being passed around. This means the expensive  context switch of whenever the thread does not get the lock hurts the waiting for lock time, in addition to the competition for the lock. The completion time per operation rises less dramatically, but still high, because the threads are still stuck waiting to acquire the lock as the thread number increases. This waiting, along with the context switches, causes the time to finally complete the operation increase. It is possible for the wait time to go up faster than the competion time because the wait time for the locks is the summation of the wait times each of the threads had to go through rather than the total single time to completion.

2.3.4
The performance of the synchronized methods greatly improves as the number of lists increases because the throughput of the program increases. This is due to the threads being able to do more work concurrently rather than having to wait for a single lock. The throughput will continue to increase as the number of lists increases up to a certain point around where numthreads=numlists because each thread will be able to work on a single list at that point with no competition with other threads for a lock. It does seem reasonable that the throughput of a N-partitioned list would be approximately the same as the throughput of a single list with fewer threads. From the graphs, we can see that the throughput of a list with 4 threads and 4 lists is approximately equivalent to that of 1 thread and 1 list. The throughput should be proportional to the ratio of the number of lists and the number of threads because each thread gets a certain number of lists to work on.